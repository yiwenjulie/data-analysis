{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study\n",
    "Twitter is a massive platform.  There are 300+ million users on Twitter, and it is a source of information for current events, social movements and, financial information.  It has been shown in a number of cases that information from Twitter can mobilize a large number of individuals.  From #blacklivesmatter to other forms of *hashtag* activism, social media can play an important role in informing and mobilizing individuals.\n",
    "\n",
    "This same activity can me extended to financial information.  The introduction of \"cashtags\" to twitter has allowed individuals to connect and discuss stocks, but it has also given stock promoters a method for promoting low value stocks, to \"pump and dump\".  Some researchers have analyzed the use of cashtags on Twitter.  We will use a similar method to look at the data, but we will ask a slightly different question.\n",
    "\n",
    "### Reading\n",
    "Hentschel M, Alonso O. 2014. Follow the money: A study of cashtags on Twitter. *First Monday*. URL: https://firstmonday.org/ojs/index.php/fm/article/view/5385/4109\n",
    "\n",
    "#### Supplementary Information\n",
    "\n",
    "* Evans, L., Owda, M., Crockett, K., & Vilas, A. F. (2019). A methodology for the resolution of cashtag collisions on Twitterâ€“A natural language processing & data fusion approach. *Expert Systems with Applications*, **127**, 353-369.\n",
    "* Evans, L., Owda, M., Crockett, K., & Vilas, A. F. (2021). [Credibility assessment of financial stock tweets](https://www.sciencedirect.com/science/article/pii/S0957417420310356). *Expert Systems with Applications*, **168**, 114351.\n",
    "* Cresci, S., Lillo, F., Regoli, D., Tardelli, S., & Tesconi, M. (2019). Cashtag Piggybacking: Uncovering Spam and Bot Activity in Stock Microblogs on Twitter. *ACM Transactions on the Web (TWEB)*, **13(2)**, 11.\n",
    "\n",
    "#### Raw Data source\n",
    "I document the source of ticker data below.  The tweet data we use here comes from a dataset used in Cresci *et al* (2019) referenced above.  The data is available through Zenodo using the dataset's DOI: [10.5281/zenodo.2686861](https://doi.org/10.5281/zenodo.2686861)\n",
    "\n",
    "### Formulating the question\n",
    "\n",
    "The question we want to ask specifically is whether *cashtag frequency is tied to increases in stock price*.\n",
    "\n",
    "To do this we need to know a few things.  First, we need to understand the frequency of cashtags, and classify them in some way.  What aspects of a cashtag are important?  What elements of a tweet containing a cashtag are important?  How do we go from raw cashtag to something we can analyze?\n",
    "\n",
    "In addition, what other information do we need to help us understand our data?  How do we know stock prices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1.  Identify elements of the potential dataset(s) that match each of the four Vs of Big Data:\n",
    "\n",
    "  a.  Velocity: \n",
    "  1. Stock trading information requires zetabyes of data to be processed in a day.\n",
    "  2. stock option prediction algorithm requires very time sensative data and quick prediction on the fly.\n",
    "\n",
    "  b.  Veracity: \n",
    "  1. issues resulting from wheather cashtags are double counted in retweets or replies; wheather mis-spelled cashtags can be identified.\n",
    "  2. poor data quality results US economy loss in trillions of dollars a year.\n",
    "\n",
    "  c.  Volume: \n",
    "  1. use cloud solution to efficiently perform calculation on Gigabytes or Terabytes of data;\n",
    "  2. Local goverment has millions of paper documents that need to be scaned and processed using NLP.\n",
    "\n",
    "  d.  Variety: \n",
    "  1. issues from stock symbol change;\n",
    "  2. Healthcare used to keep records in sql server, excel, text file and paper binders.\n",
    "  \n",
    "\n",
    "2. To find the stock price at a point in time for a cashtag (e.g., $A), we need to know which company uses that NYSE listing, and then find the listing at that time period.  A public dataset of stock listings is available at [ftp.nasdaqtrader.com/SymbolDirectory/otherlisted.txt]().  \n",
    "\n",
    "  a. You can download files directly into Python from an FTP service using the command FTP.  Read the file into Python and return the number of rows, and return the symbol of the stock ticker associated with the longest named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "# The following lines are for your information.  they represent a recipe for downloading data directly from an FTP server:\n",
    "session = FTP('ftp.nasdaqtrader.com')\n",
    "session.login()\n",
    "r = StringIO()\n",
    "session.retrlines('RETR /SymbolDirectory/otherlisted.txt', lambda x: r.write(x+'\\n'))\n",
    "\n",
    "r.seek(0)\n",
    "csvfile = list(csv.DictReader(r, delimiter='|'))\n",
    "\n",
    "# From here apply your solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5782"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output number of rows\n",
    "n=len(csvfile);n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GJP'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longestname=csvfile[0][\"Security Name\"]\n",
    "\n",
    "for i in range(n):\n",
    "    if len(csvfile[i][\"Security Name\"])>len(longestname):\n",
    "        a=i\n",
    "        longestname=csvfile[i][\"Security Name\"]\n",
    "csvfile[a][\"ACT Symbol\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b.  In the database there is a table called `tickers`.  Connect to the database using Python.  Using a SQL query, return the number of rows in this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5754,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "conString = {'host':os.environ.get('DB_HOST'),\n",
    "             'dbname':os.environ.get('DB_NAME'),\n",
    "             'user':os.environ.get('DB_USER'),\n",
    "             'password':os.environ.get('DB_PASS'),\n",
    "             'port':os.environ.get('DB_PORT')}\n",
    "conn = psycopg2.connect(**conString)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"select count(*) from import.tickers\")\n",
    "nrow=cur.fetchone()\n",
    "nrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c. Use a SQL query (with Python) to return the row with the longest company name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GJP',\n",
       " 'Synthetic Fixed-Income Securities, Inc. Synthetic Fixed-Income Securities, Inc. on behalf of STRATS (SM) Trust for Dominion Resources, Inc. Securities, Series 2005-6, Floating Rate Structured Repackaged Asset-Backed Trust Securities (STRATS) Certificates',\n",
       " 'N',\n",
       " 'GJP',\n",
       " 'N',\n",
       " '100',\n",
       " 'N',\n",
       " 'GJP')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conn.close()\n",
    "conn = psycopg2.connect(**conString)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT *\n",
    "FROM import.tickers\n",
    "ORDER BY length(securityname) desc\n",
    "Limit 1\"\"\")\n",
    "cur.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. The output of an individual tweet may be a complex object, returned from the Twitter API.  This data is stored as a JSON object within a Postgres database in the cloud.  The table is in a schema called `import` in a table called `tweets`.  Connect to the database.  How many individual tweets are in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn.close()\n",
    "#conn = psycopg2.connect(**conString)\n",
    "#cur = conn.cursor()\n",
    "cur.execute(\"select count(distinct id) from import.tweets\")\n",
    "nrow=cur.fetchone()\n",
    "nrow\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Currently all columns in the table `import.tweets` are coded as Postgres `text` columns.  How would you normalize these tables?  Use [`CREATE TABLE IF EXISTS`]() to generate the appropriate tables in the `import` schema.  What do the normalized tables look like? You should add PRIMARY and FOREIGN key restraints, but do not need to add indexes or other constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a users table containing userid data\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS as import.users (\n",
    "userid numeric PRIMARY KEY);) \"\"\")\n",
    "\n",
    "#create a tweets table containing tweet id, content, retweet and reply.\n",
    "#use foreign keys to point to the same tweets table to create links between tweets,retweets and replies.\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS as import.tweets (\n",
    "id numeric PRIMARY KEY,\n",
    "content text,\n",
    "inreplytostatusid numeric REFERENCES schema.tweet(id),\n",
    "inreplytouserid numeric REFERENCES schema.tweet(userid),\n",
    "retweetedstatusid numeric REFERENCES schema.tweet(id),\n",
    "retweeteduserid numeric REFERENCES schema.tweet(userid),\n",
    "lang text,\n",
    "source text,\n",
    "createdat timestamp,\n",
    "userid numeric REFERENCES schema.user(userid));) \"\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "affiliation": "UBC MBAN2021",
    "email": "jedyfun@gmail.com",
    "name": "Jerry Shao"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
